{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is THE model.\n",
    "\n",
    "It combines improvements from the LLAMA3 series, with others from Gemma2.\n",
    "It has:\n",
    "- GQA\n",
    "- GeGLU (thinking to maybe use SwiGLU or ReGLU)\n",
    "- RoPE\n",
    "\n",
    "I still would need to implement KV-caching to improve inference type.\n",
    "\"\"\"\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import inspect\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "class CausalSelfAttentionGQA(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        self.head_dim = config.n_embd // config.n_head\n",
    "        self.n_kv_heads = config.n_kv_heads # Nombre de grups de query\n",
    "        self.n_head = config.n_head\n",
    "\n",
    "        shape = (config.n_head + 2 * config.n_kv_heads) * self.head_dim\n",
    "\n",
    "        self.wq = nn.Linear(config.n_embd, config.n_head * self.head_dim, bias=False )\n",
    "        self.wk = nn.Linear(config.n_embd, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(config.n_embd, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(config.n_head * self.head_dim, config.n_embd, bias=False)\n",
    "        self.cache = None\n",
    "        self.queries_per_kv = self.n_head // self.n_kv_heads\n",
    "\n",
    "    def forward(self, x, freqs_cis, mask, return_attention=False):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "        xq = xq.view(B, T, self.n_head, self.head_dim)\n",
    "        xk = xk.view(B, T, self.n_kv_heads, self.head_dim)\n",
    "        xv = xv.view(B, T, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        xq = apply_rope(xq, freqs_cis)\n",
    "        xk = apply_rope(xk, freqs_cis)\n",
    "\n",
    "        xk = repeat_kv(xk, self.queries_per_kv)\n",
    "        xv = repeat_kv(xv, self.queries_per_kv)\n",
    "        print(\"aquí si\")\n",
    "        \n",
    "        xq, xk, xv = (x.transpose(1, 2) for x in (xq, xk, xv))\n",
    "\n",
    "        scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, xv)\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous().view(B, T, -1)\n",
    "        # output projection\n",
    "        proj = self.wo(output)\n",
    "        if return_attention:\n",
    "            return proj, scores\n",
    "        return proj\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rope(q, k, cos, sin, unsqueeze_dim=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "class CausalSelfAttentionGQA(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        self.config = config\n",
    "        self.num_heads = config.n_head\n",
    "        self.num_kv_heads = config.n_kv_heads\n",
    "        self.head_dim = config.n_embd // self.num_heads\n",
    "        self.num_kv_groups = self.num_heads // self.num_kv_heads\n",
    "        \n",
    "\n",
    "        self.q_proj = nn.Linear(config.n_embd, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(config.n_embd, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.n_embd, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, config.n_embd, bias=False)\n",
    "\n",
    "\n",
    "        #self.sliding_window_size = config.sliding_window_size\n",
    "        self.max_seq_len = config.block_size\n",
    "\n",
    "    def forward(self, x, cos_sin: tuple, return_attention = None):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        k = k.view(B, T, self.num_kv_heads, self.head_dim).transpose(1,2)\n",
    "        v = v.view(B, T, self.num_kv_heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        cos, sin = cos_sin\n",
    "        q, k = apply_rope(q, k, cos, sin)\n",
    "\n",
    "        k = repeat_kv(k, self.num_kv_groups)\n",
    "        v = repeat_kv(v, self.num_kv_groups)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(2,3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        mask = True\n",
    "        if mask is not None:  # the mask is not correct, it needs to be [B, C, T, T]\n",
    "            mask = torch.full((T, T), float(\"-inf\"), device=x.device)\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            mask = mask.unsqueeze(0).unsqueeze(1)\n",
    "            scores = scores + mask\n",
    "\n",
    "        scores = nn.functional.softmax(scores, dim=-1, dtype=torch.float32).to(q.dtype)\n",
    "        output = torch.matmul(scores, v)\n",
    "\n",
    "        if output.size() != (B, self.num_heads, T, self.head_dim):\n",
    "            raise ValueError(f\"ALGO HA ANAT MALAMENT, output té dimensions {output.size()}\")\n",
    "        \n",
    "        output = output.transpose(1,2).contiguous()\n",
    "        output = output.reshape(B, T, -1)\n",
    "\n",
    "        output = self.o_proj(output)\n",
    "\n",
    "        if return_attention:\n",
    "            return output, scores\n",
    "        return output, None\n",
    "        \n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d)) # weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_dtype = x.dtype\n",
    "        x = x.to(torch.float32)\n",
    "        variance = x.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = x * torch.rsqrt(variance + self.eps)\n",
    "        return self.weight * hidden_states.to(input_dtype)\n",
    "\n",
    "\n",
    "def compute_rope_default(config, device):\n",
    "    base = config.rope_theta\n",
    "    dim = config.n_embd // config.n_head\n",
    "\n",
    "    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float().to(device)/dim))\n",
    "    return inv_freq, 1.0\n",
    "\n",
    "\n",
    "\n",
    "class LlamaRotaryEmbedding(nn.Module):\n",
    "    def __init__(self, config, device = None):\n",
    "        super().__init__()\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.factor = config.scaling_factor\n",
    "        self.base = config.rope_theta\n",
    "        self.dim = config.n_embd // config.n_head\n",
    "        self.config = config\n",
    "        self.rope_init_fn = compute_rope_default\n",
    "\n",
    "        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        self.original_inv_freq = self.inv_freq\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "\n",
    "        freqs = (inv_freq_expanded @ position_ids_expanded).transpose(1, 2)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        cos = emb.cos()\n",
    "        sin = emb.sin()\n",
    "\n",
    "        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n",
    "        cos = cos * self.attention_scaling\n",
    "        sin = sin * self.attention_scaling\n",
    "\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
    "\n",
    "        \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    class GeGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gate = x.chunk(2, dim=-1)\n",
    "        # Silu és el mateix que swift function\n",
    "        return F.gelu(gate) * x\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.gate_proj   = nn.Linear(config.n_embd, config.intermediate_size, bias=False) # gate_proj\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.n_embd, bias=False) # down proj\n",
    "        self.up_proj = nn.Linear(config.n_embd, config.intermediate_size, bias=False) # up proj\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.down_proj(F.gelu(self.gate_proj(x), approximate=\"tanh\")* self.up_proj(x)) \n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.input_layernorm = RMSNorm(config.n_embd, config.norm_eps) # input_layernorm\n",
    "        self.self_attn = CausalSelfAttentionGQA(config)\n",
    "        self.post_attention_layernorm = RMSNorm(config.n_embd, config.norm_eps) # post_attention_layernorm\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x, cos_sin):\n",
    "        residual = x\n",
    "        x = self.input_layernorm(x)\n",
    "        x, attn_weights = self.self_attn(x, cos_sin)\n",
    "\n",
    "        x = residual + x\n",
    "\n",
    "        residual = x\n",
    "        x = self.post_attention_layernorm(x)\n",
    "        x = self.mlp(x)\n",
    "        x = residual + x\n",
    "\n",
    "        return x\n",
    "\n",
    "class Aloja(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.embed_tokens =nn.Embedding(config.vocab_size, config.n_embd, self.padding_idx)\n",
    "\n",
    "        self.layers = nn.ModuleList([Block(config) for i in range(config.n_layer)])\n",
    "\n",
    "        self.norm =  RMSNorm(config.n_embd, config.norm_eps)\n",
    "        self.rotary_emb = LlamaRotaryEmbedding(config=config)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        self.embed_tokens.weight = self.lm_head.weight # Linkeddddd\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is smaller\"\n",
    "\n",
    "        x = self.embed_tokens(idx)\n",
    "\n",
    "\n",
    "        position_ids = torch.arange(T, device = x.device).unsqueeze(0)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        cos_sin = self.rotary_emb(x, position_ids)\n",
    "\n",
    "        for block in self.layers:\n",
    "            x = block(x, cos_sin)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "\n",
    "        logits = self.lm_head(x).float()\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def configure_optimizers(self, learning_rate, weight_decay=0.1, betas=(0.9, 0.95), device_type='cuda'):\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == \"cuda\"\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, eps=1e-8, fused=use_fused)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AlojaConfig:\n",
    "    block_size: int = 2048\n",
    "    vocab_size: int = 65536\n",
    "    n_layer: int = 30\n",
    "    n_head: int = 8\n",
    "    n_embd: int = 768 # hidden_size\n",
    "    intermediate_size = 2048 # 3072\n",
    "    n_kv_heads: int = 4 # nombre de grups de query\n",
    "    norm_eps: int = 1e-05\n",
    "    rope_theta: float = 10000.0\n",
    "    use_scaled_rope: bool = False\n",
    "    scaling_factor: int = 1\n",
    "    max_batch_size: int = 16\n",
    "    max_seq_len:int = 2048\n",
    "    max_position_embeddings:int = 2048\n",
    "    pad_token_id:int = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, dataloader, checkpoint_path, device):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.config(checkpoint['config'])\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    step = checkpoint['step']\n",
    "    print(f\"Checkpoint loaded from step {step}\")\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_8896\\3430835681.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"./model_02500.pt\", map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"./model_02500.pt\", map_location=\"cpu\")\n",
    "model = Aloja(checkpoint['config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = checkpoint['model']\n",
    "new_state_dict = {}\n",
    "for key, value in state_dict.items():\n",
    "    new_key = key.replace(\"_orig_mod.\", \"\")\n",
    "    new_state_dict[new_key] = value\n",
    "\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m max_lenght:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 34\u001b[0m         logits, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m         logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m     36\u001b[0m         probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 303\u001b[0m, in \u001b[0;36mAloja.forward\u001b[1;34m(self, idx, targets)\u001b[0m\n\u001b[0;32m    300\u001b[0m cos_sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(x, position_ids)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 303\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos_sin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m    307\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 247\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x, cos_sin)\u001b[0m\n\u001b[0;32m    245\u001b[0m residual \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    246\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(x)\n\u001b[1;32m--> 247\u001b[0m x, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos_sin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m x \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m x\n\u001b[0;32m    251\u001b[0m residual \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 153\u001b[0m, in \u001b[0;36mCausalSelfAttentionGQA.forward\u001b[1;34m(self, x, cos_sin, return_attention)\u001b[0m\n\u001b[0;32m    150\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    151\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mreshape(B, T, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 153\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mo_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_attention:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output, scores\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "enc = Tokenizer.from_file(\"./tokenizer/byte-level-bpe.tokenizer.json\")\n",
    "\n",
    "\n",
    "num_return_sequences = 10\n",
    "max_lenght = 1024\n",
    "\n",
    "tokens = np.array(enc.encode(\"Hola, sóc un model de llenguatge,\").ids)\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "#x = tokens.to('cuda')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "x = tokens.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "while x.size(1) < max_lenght:\n",
    "    with torch.no_grad():\n",
    "        logits, _ = model(x)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        ix = torch.multinomial(topk_probs, 1)\n",
    "        xcol = torch.gather(topk_indices, -1, ix)\n",
    "        x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = x[i, :max_lenght].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola, sóc un model de llenguatge, i avui he donat voltes a les coses que vull expressar quan un nen diu que li encanten. No he tingut mai aquesta visió d'estar sol en un ambient familiar sense una bona disposició a l'aula, ni coneixements mínims per ensenyar-l i que sovint no se sol ni se sent gaire complicat trobar solucions a la seva pròpia situació.\n",
      "Doncs això és exactament el que vull dir. Perquè és important fer reflexionar a l'infant, perquè, si el seu pensament no és coherent, l'ajuda a trobar ajuda per deixar-se endur, llavors és tan important. I aquesta implicació és una part important de la responsabilitat de la intel·ligència. Per això, he\n"
     ]
    }
   ],
   "source": [
    "tokens = x[2, :max_lenght].tolist()\n",
    "decoded = enc.decode(tokens)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlojaConfig(block_size=2048, vocab_size=65536, n_layer=30, n_head=8, n_embd=768, n_kv_heads=4, norm_eps=1e-05, rope_theta=10000.0, use_scaled_rope=False, scaling_factor=1, max_batch_size=16, max_seq_len=2048, max_position_embeddings=2048, pad_token_id=3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaConfig,  LlamaForCausalLM\n",
    "\n",
    "configuration = LlamaConfig(\n",
    "    vocab_size = 65536,\n",
    "    hidden_size = 768,\n",
    "    intermediate_size = 2048,\n",
    "    num_hidden_layers = 30,\n",
    "    num_attention_heads= 8,\n",
    "    num_key_value_heads= 4,\n",
    "    hidden_act = \"gelu\",\n",
    "    max_position_embeddings = 2048,\n",
    "    initializer_range = 0.02,\n",
    "    rms_norm_eps = 1e-05,\n",
    "    tie_word_embeddings= True,\n",
    "    rope_theta = 10000.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaForCausalLM(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = checkpoint['model']\n",
    "new_state_dict = {}\n",
    "for key, value in state_dict.items():\n",
    "    new_key = key.replace(\"_orig_mod.\", \"model.\")\n",
    "    if new_key == \"model.lm_head.weight\":\n",
    "        new_key = \"lm_head.weight\"\n",
    "    new_state_dict[new_key] = value\n",
    "\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"./tokenizer/byte-level-bpe.tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "from transformers import EosTokenCriteria, StoppingCriteriaList\n",
    "genconf = GenerationConfig(\n",
    "    do_sample = True,\n",
    "    max_length = 500,\n",
    "    top_k = 50,\n",
    "    num_return_sequences = 1,\n",
    "    eos_token_id = 1,\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.9,\n",
    "    pad_token_id = 3,\n",
    "    repetition_penalty = 1.5,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = np.array(enc.encode(\"Aquest matí he anat\").ids)\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0)\n",
    "\n",
    "a = model.generate(tokens, genconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">  Hi havia una vegada un noi anomenat Roger Baiges. Un bon dia, en Roger osset va veure el seu avi al bosc i li digué: \"Per què has vingut aquí?\"\n",
      "\"Aquest és l'avi de la iaia\", deia Pau amb veu tremolosa mentre intentava parlar-li a ell mateix per telèfon des d'un racó del jardí on es trobava amagat sota terra. En Pere no sabia com arribar allà! Va mirar cap amunt pel camí que conduïa fins dalt dels arbres alts i frondosos. La seva mirada era baixa però ferma alhora. El riu brillava més fort quan els ocells cantaven melodies estranyes durant hores sense parar atenció als seus moviments. No podia deixar escapar aquesta sensació tan trista ni màgica perquè semblava impossible trobar algú desconegut dins d'una caixa misteriosa amagada entre les arrels d'aquest arbre màgic.  \n",
      "\n",
      ">  Hi havia una vegada un noi anomenat Roger Baiges. Un bon dia, en Roger  va veure a l'home assegut al seu costat amb el dit índex i li deia: \"El meu nom és Oliver\".\n",
      "En Jordi es feia passar per home d'una altra època que no era tan bo com ell pensava perquè sempre tenia ganes de jugar. Va anar-hi quan van descobrir la veritat sobre els seus pares, però sense adonar-se del què passava realment entre ells ni dels fets reals que hi vivien. En Miquel ja estava molt content i finalment ho veia tot perfecte!\n",
      ">  Hi havia una vegada un noi anomenat Roger Baiges. Un bon dia, en Roger  va anar a l'escola i li van ensenyar les paraules de la seva mare: \"La màgia està amagada\".\n",
      "En Pere es va posar molt content perquè el seu pare tenia raó! Li agradava ajudar els altres nens amb molta il·lusió. I no només això sinó que ell també volia ser com ells, ajudant-los cada cop més ràpid al món màgic del nen petit.\" En Pau estava sorprès per aquesta idea tan divertida d'ajudar als seus amics. La mestra deia que era important compartir aquest coneixement amb tots aquells que tenen por dels problemes reals o de veure's obligats a treballar junts si volen aprendre alguna cosa nova.  \n",
      "\n",
      ">  Hi havia una vegada un noi anomenat Roger Baiges. Un bon dia, en Roger  va veure el seu amic i li digué: \"El meu millor amic és l'avi\".\n",
      "-És que jo no sóc fill de Gaziel! -va dir la seva amiga amb entusiasme al gran Pere del poble (que era molt famós). El Jaume es deia Josep Maria Jujol i ell estava casat amb Berta González. En Manel tenia dos fills més grans que ella mateixa per ser mestre d'obres públiques a Catalunya durant els anys 70 com també ho eren altres pares dels seus germans petits. L'àvia sempre explicava històries sobre ells mateixos perquè havien conegut algú tan importantíssim i tenien molts problemes econòmics. La família treballava sense cap tipus d'educació especial ni formació professional o religiosa; així doncs van començar les seves vides treballant duríssimes coses fins ara desconegudes pel món sencer... Va aconseguir fama mundial gràcies als diners rebuts pels bancs locals i des de llavors s'han convertit encara avui en importants fortunes privades catalanes.\n",
      ">  Hi havia una vegada un noi anomenat Roger Baiges. Un bon dia, en Roger  va anar a buscar el seu pare per ajudar-lo i li van dir que era molt bo amb ell: \"Oh! On ets?\"\n",
      "En Jordi es deia Martí de seguida i volia saber més sobre la seva mare al costat del camp d'extermini nazi. Li encantava llegir llibres antics com El soldat malalt o La vida secreta dels morts vivents... Va començar explicant contes increïbles on els nazis deien que no podia ser feliç si s'havia perdut l´amor entre ells perquè havien comès crims contra persones innocents. En aquest llibre explica històries divertides, però també reals i sorprenents (com ara les seves aventures). A través d'aquest conte, ens endinsarem en la història real d'un nen innocent que vol tenir èxit quan ha mort pel camí correcte.\n"
     ]
    }
   ],
   "source": [
    "for response in range(len(a)):\n",
    "    print(\"> \", tokenizer.decode(a[response].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_8896\\3483756613.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_4 = torch.load(\"./model_11000.pt\", map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_4 = torch.load(\"./model_11000.pt\", map_location=\"cpu\")\n",
    "state_dict_4 = checkpoint_4['model']\n",
    "new_state_dict_4 = {}\n",
    "for key, value in state_dict_4.items():\n",
    "    new_key = key.replace(\"_orig_mod.\", \"model.\")\n",
    "    if new_key == \"model.lm_head.weight\":\n",
    "        new_key = \"lm_head.weight\"\n",
    "    new_state_dict_4[new_key] = value\n",
    "\n",
    "\n",
    "from transformers import LlamaConfig,  LlamaForCausalLM\n",
    "\n",
    "configuration = LlamaConfig(\n",
    "    vocab_size = 65536,\n",
    "    hidden_size = 768,\n",
    "    intermediate_size = 2048,\n",
    "    num_hidden_layers = 30,\n",
    "    num_attention_heads= 8,\n",
    "    num_key_value_heads= 4,\n",
    "    hidden_act = \"gelu\",\n",
    "    max_position_embeddings = 2048,\n",
    "    initializer_range = 0.02,\n",
    "    rms_norm_eps = 1e-05,\n",
    "    tie_word_embeddings= True,\n",
    "    rope_theta = 10000.0\n",
    ")\n",
    "model_4 = LlamaForCausalLM(configuration)\n",
    "\n",
    "model_4.load_state_dict(new_state_dict_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">  Hi havia una vegada un gat que volia ser cuiner. Un dia, va veure un ratolí petit i gris amb els ulls brillants a la seva cua. El ratolí tenia molta gana! \n",
      "\n",
      "\"Hola gatet!\" Va dir el ratolí amb veu dolça. \"M'agradaria ajudar-te a trobar menjar.\"\n",
      "\n",
      "El ratolí es va posar molt content de saber què era el peixet blau. Va sortir corrent cap al bosc encantat on vivia la Bruixa Lila. Ella li va explicar tot sobre les galetes màgiques del llac. La bruixa va escoltar atentament, però no va poder evitar preguntar: \"Què són aquestes galetes?\"\n",
      "\n",
      "La Marta, sorpresa, va mirar el ratolí blau. Era tan gran com ella! I quin sabor feia?\n",
      "\n",
      "\"Aquestes són galetes per als humans,\" va continuar el ratolí. \"Són com petites notes que diuen 'Benvingut al Bosc Encantat'. Són com petits missatges secrets que ens ajuden a descobrir coses noves!\"\n",
      "\n",
      "La Marta va quedar fascinada. No sabia que hi hagués tants tipus diferents d'aliments! Però també estava emocionada per aprendre més sobre aquest món màgic.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "genconf = GenerationConfig(\n",
    "    do_sample = True,\n",
    "    max_length = 350,\n",
    "    top_k = 50,\n",
    "    num_return_sequences = 1,\n",
    "    eos_token_id = 1,\n",
    "    temperature = 0.6,\n",
    "    top_p = 0.9,\n",
    "    pad_token_id = 3,\n",
    "    repetition_penalty = 1.2,\n",
    "\n",
    ")\n",
    "tokens = np.array(enc.encode(\"Hi havia una vegada\").ids)\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0)\n",
    "a = model_4.generate(tokens, genconf)\n",
    "for response in range(len(a)):\n",
    "    print(\"> \", tokenizer.decode(a[response].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: './tokenizer/byte-level-bpe.tokenizer.json'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\utils\\_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\utils\\_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './tokenizer/byte-level-bpe.tokenizer.json'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[328], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./tokenizer/byte-level-bpe.tokenizer.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:826\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    825\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[1;32m--> 826\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[0;32m    828\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:658\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m    655\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m    657\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 658\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    675\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\hub.py:466\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    467\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    468\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[1;31mOSError\u001b[0m: Incorrect path_or_model_id: './tokenizer/byte-level-bpe.tokenizer.json'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./tokenizer/byte-level-bpe.tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"./tokenizer/byte-level-bpe.tokenizer.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6437ba2b81c44f68b90d8834e7137845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d0ef1f390a40ee959b894b37f048b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/980M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'tokenizers.Tokenizer' object has no attribute 'push_to_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[324], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m repo_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpauhidalgoo/cucafera\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mpush_to_hub(repo_name)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m(repo_name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tokenizers.Tokenizer' object has no attribute 'push_to_hub'"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, HfFolder, Repository, notebook_login\n",
    "\n",
    "notebook_login()\n",
    "\n",
    "repo_name = \"pauhidalgoo/cucafera\"\n",
    "model.push_to_hub(repo_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e900f5ba209249d49fe267d536c6dba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae8d1b1b9674eb2964d854223a68d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Usuario\\.cache\\huggingface\\hub\\models--pauhidalgoo--cucafera. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/pauhidalgoo/cucafera/commit/ba154d6a0028da9fc30eace50bb0fe9be49e7e47', commit_message='Upload tokenizer', commit_description='', oid='ba154d6a0028da9fc30eace50bb0fe9be49e7e47', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, HfFolder, Repository, notebook_login\n",
    "\n",
    "notebook_login()\n",
    "wrapped_tokenizer.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "moodel = AutoModelForCausalLM.from_pretrained(\"./acontinua\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">  La intel·ligència artificial tindrà la capacitat de generar noves idees i solucions a partir d'un conjunt complex de dades. Aquesta capacitat obre un ventall de possibilitats per a l'aprenentatge automàtic, permetent-nos explorar patrons complexos en el món real sense necessitat de ser programats explícitament.\n",
      "Un dels aspectes més importants del potencial de la IA per al desenvolupament de productes és la seva capacitat per a generar noves idees. La IA pot analitzar grans quantitats de dades, identificar patrons i tendències que no es poden capturar amb precisió, com ara els models de predicció de mercat o les xarxes socials. Això permet als desenvolupadors crear productes que siguin fàcils d'entendre i aplicar, cosa que pot conduir a una major eficiència i rendibilitat. \n",
      "Per exemple, imagineu un robot que ha de navegar pel bosc. El seu objectiu principal seria trobar un equilibri entre la seguretat i la satisfacció del client. En lloc de buscar informació sobre arbres, podria utilitzar algoritmes d'intel·ligència artificial per a predir possibles problemes futurs i prendre decisions estratègiques basades en la probabilitat. Aquest procés de raonament s'anomena aprenentatge profund.\n",
      "A més de la IA, la IA també té aplicacions pràctiques en diversos camps:\n",
      " Finances: Els sistemes financers basats en IA permeten als inversors invertir en empreses que tenen un alt risc financer. Per exemple, els bancs utilitzen algorismes d'IA per a estimar el valor futur de les seves inversions, mentre que els fons d'inversió busquen maximitzar els seus beneficis.\n",
      " Comerç electrònic: Les plataformes de comerç electrònic ofereixen eines poderoses per a gestionar el trànsit web, optimitzar el rendiment del sistema i millorar la qualitat del contingut. Aquestes eines són essencials per a garantir la fiabilitat i la transparència del servei al client.\n",
      " Educació: L'ús de la IA en educació pot ajudar els\n"
     ]
    }
   ],
   "source": [
    "genconf = GenerationConfig(\n",
    "    max_length = 350,\n",
    "    repetition_penalty = 1.2,\n",
    "\n",
    ")\n",
    "tokens = np.array(enc.encode(\"La intel·ligència artificial tindrà la capacitat de\").ids)\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0)\n",
    "a = moodel.generate(tokens, genconf)\n",
    "for response in range(len(a)):\n",
    "    print(\"> \", tokenizer.decode(a[response].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval.hellaswag import iterate_examples, render_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_likely_row(tokens, mask, logits):\n",
    "    \"\"\"\n",
    "    Funció per triar quina de les opcions múltiples escull el model\n",
    "    (es fa servir per avaluar-lo utilitzant hellaswag)\n",
    "    \"\"\"\n",
    "    # evaluate the autoregressive loss at all positions\n",
    "    shift_logits = (logits[..., :-1, :]).contiguous()\n",
    "    shift_tokens = (tokens[..., 1:]).contiguous()\n",
    "    flat_shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "    flat_shift_tokens = shift_tokens.view(-1)\n",
    "    shift_losses = F.cross_entropy(flat_shift_logits, flat_shift_tokens, reduction='none')\n",
    "    shift_losses = shift_losses.view(tokens.size(0), -1)\n",
    "    # now get the average loss just for the completion region (where mask == 1), in each row\n",
    "    shift_mask = (mask[..., 1:]).contiguous().to(logits.device) # we must shift mask, so we start at the last prompt token\n",
    "    masked_shift_losses = shift_losses * shift_mask\n",
    "    # sum and divide by the number of 1s in the mask\n",
    "    sum_loss = masked_shift_losses.sum(dim=1)\n",
    "    avg_loss = sum_loss / shift_mask.sum(dim=1)\n",
    "    # now we have a loss for each of the 4 completions\n",
    "    # the one with the lowest loss should be the most likely\n",
    "    pred_norm = avg_loss.argmin().item()\n",
    "    return pred_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x8d in position 883: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m num_correct_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      2\u001b[0m num_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterate_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrender_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Documents\\Projectes\\Minerva\\minerva\\eval\\hellaswag.py:129\u001b[0m, in \u001b[0;36miterate_examples\u001b[1;34m(split)\u001b[0m\n\u001b[0;32m    127\u001b[0m download(split)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_CACHE_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhellaswag_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 129\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mcharmap_decode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,decoding_table)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x8d in position 883: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "num_correct_norm = 0\n",
    "num_total = 0\n",
    "for i, example in enumerate(iterate_examples(\"val\")):\n",
    "    _, tokens, mask, label = render_example(example)\n",
    "    tokens = tokens.to(device)\n",
    "    maks = mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens)\n",
    "        logits = outputs.logits\n",
    "        pred_norm = get_most_likely_row(tokens, mask, logits)\n",
    "    num_total += 1\n",
    "    num_correct_norm += int(pred_norm == label)\n",
    "acc_norm = num_correct_norm / num_total\n",
    "print(f\"HellaSwag accuracy: {num_correct_norm}/{num_total} = {acc_norm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7af64b99d1a4046bd0f0dd93b672bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Usuario\\.cache\\huggingface\\hub\\models--pauhidalgoo--cucafera-chat. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a5d8e0907b44a4bf4bae95085f23d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.88M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7c0a65977c4a8aacfc36fb9e13b01a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ac2def668549b3b164a04dc3d1e643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/729 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19fd4f89cf54c6587f68eb7277b583b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/980M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e428358a4a4ab19c53755a76e5f59a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/144 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pauhidalgoo/cucafera-chat\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"pauhidalgoo/cucafera-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">  <|im_start|>user\n",
      "Hola!<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Bon dia!<|im_end|>\n",
      "<|im_start|>user\n",
      "Què és un pingüí?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Un pingüí és una figura molt peculiar, però no és un animal.  És un mamífer?<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "genconf = GenerationConfig(\n",
    "    max_length = 150,\n",
    "    repetition_penalty = 1.2,\n",
    "    temperature = 0.6,\n",
    "    top_k = 50,\n",
    "    top_p = 0.9,\n",
    "    do_sample = True,\n",
    "\n",
    ")\n",
    "tokens = np.array(tokenizer.encode(\"\"\"<|im_start|>user\n",
    "Hola!<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Bon dia!<|im_end|>\n",
    "<|im_start|>user\n",
    "Què és un pingüí?<|im_end|>\n",
    "<|im_start|>assistant\"\"\"))\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0)\n",
    "a = model.generate(tokens, genconf)\n",
    "for response in range(len(a)):\n",
    "    print(\"> \", tokenizer.decode(a[response].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "tokenizer_instruct = AutoTokenizer.from_pretrained(\"pauhidalgoo/cucafera-instruct\")\n",
    "model_instruct = AutoModelForCausalLM.from_pretrained(\"pauhidalgoo/cucafera-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">  <|im_start|>user\n",
      " Fes una endevinalla<|im_end|>\n",
      "<|im_start|>assistant\n",
      "No hi ha cap dia més a la vida. Hi havia un noi anomenat Jordi que vivia sol al poble d'Elvis, mentre feia el seu esmorzar de pa amb tomàquet i suc de taronja.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "genconf = GenerationConfig(\n",
    "    max_length = 150,\n",
    "    repetition_penalty = 1.2,\n",
    "    temperature = 0.6,\n",
    "    top_k = 50,\n",
    "    top_p = 0.9,\n",
    "    do_sample = True,\n",
    "\n",
    ")\n",
    "tokens = np.array(tokenizer_instruct.encode(\"<|im_start|>user\\n Fes una endevinalla<|im_end|>\\n<|im_start|>assistant\\n\"))\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0)\n",
    "a = model_instruct.generate(tokens, genconf)\n",
    "for response in range(len(a)):\n",
    "    print(\"> \", tokenizer_instruct.decode(a[response].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "user\n",
      " Quines són les lleis de Newton \n",
      "assistant\n",
      "Les lleis de Newton són:\n",
      "\n",
      "1. La Llei de Newton: Aquesta llei estableix que el moviment d'un objecte és proporcional al quadrat de la seva massa.\n",
      "2. La Llei de Newton: Aquesta llei estableix que la força de la gravetat és directament proporcional al quadrat de la massa.\n",
      "3. La Llei de Newton: Aquesta llei estableix que la força de la gravetat és directament proporcional al quadrat de la massa.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \" Quines són les lleis de Newton \"},\n",
    "]\n",
    "\n",
    "# prepare the messages for the model\n",
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "\n",
    "# inference\n",
    "outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    ")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
