{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is THE model.\n",
    "\n",
    "It combines improvements from the LLAMA3 series, with others from Gemma2.\n",
    "It has:\n",
    "- GQA\n",
    "- Sliding window attention every two layers\n",
    "- GeGLU (thinking to maybe use SwiGLU or ReGLU)\n",
    "- RoPE\n",
    "\n",
    "I still would need to implement KV-caching to improve inference type.\n",
    "\"\"\"\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import inspect\n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "\n",
    "\n",
    "class CausalSelfAttentionGQA(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        self.head_dim = config.n_embd // config.n_head\n",
    "\n",
    "\n",
    "        shape = (config.n_head + 2 * config.n_kv_heads) * self.head_dim\n",
    "\n",
    "        self.c_attn = nn.Linear(config.n_embd, shape, bias=False) # q_proj, k_proj i v_proj joined\n",
    "        self.o_proj = nn.Linear(self.head_dim * config.n_head, config.n_embd, bias=False) # o_proj\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        \n",
    "\n",
    "        self.n_kv_heads = config.n_kv_heads # Nombre de grups de query\n",
    "\n",
    "        assert self.n_head % self.n_kv_heads == 0, \"n_head must be divisible by n_group\"\n",
    "        self.queries_per_kv = self.n_head // self.n_kv_heads # n_rep\n",
    "\n",
    "        #self.sliding_window_size = config.sliding_window_size\n",
    "        self.max_seq_len = config.block_size\n",
    "\n",
    "    def forward(self, x, freqs_cis):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        qkv = self.c_attn(x)\n",
    "        total_qkv = self.queries_per_kv + 2 # cada grup té a més de queries, 1 key i 1 value\n",
    "        qkv = qkv.view(B, T, self.n_kv_heads, total_qkv, self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 3, 1, 4)\n",
    "\n",
    "        q, k, v = qkv.split((self.queries_per_kv, 1, 1), dim=2)\n",
    "\n",
    "        q = q.reshape(B, -1, self.n_head, self.head_dim)  # (B, T, n_h, hs)\n",
    "        k = k.reshape(B, -1, self.n_kv_heads, self.head_dim)  # (B, T, nh_kv, hs)\n",
    "        v = v.reshape(B, -1, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        q = apply_rope(q, freqs_cis)\n",
    "        k = apply_rope(k, freqs_cis)\n",
    "\n",
    "        if self.n_kv_heads != self.n_head:\n",
    "            k = repeat_kv(k, n_rep=self.queries_per_kv)\n",
    "            v = repeat_kv(v, n_rep=self.queries_per_kv)\n",
    "\n",
    "        # (B, n_h, T, hs)\n",
    "        q = q.transpose(1,2)\n",
    "        k = k.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "        \"\"\"\n",
    "            SLIDING WINDOW DISCARDED (FOR NOW) TO MATCH EXACTLY LLAMAFORCAUSALLM\n",
    "            scores = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "            all_ones = torch.ones((T, T), device=q.device)\n",
    "            sliding_mask = torch.triu(all_ones, -self.sliding_window_size + 1) * torch.tril(all_ones, self.sliding_window_size - 1)\n",
    "            sliding_mask = sliding_mask.unsqueeze(0).unsqueeze(0)\n",
    "            mask = torch.where(sliding_mask == 1, torch.zeros_like(scores), torch.full_like(scores, float(\"-inf\")))\n",
    "            scores = scores + mask\n",
    "            scores = F.softmax(scores.float(), dim=-1).type_as(q)\n",
    "            # [B, n_h, T, hs]\n",
    "            y = torch.matmul(scores, v)\n",
    "        \"\"\"\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.o_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(d)) # weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = torch.sqrt(x.float().pow(2).mean(-1, keepdim=True) + self.eps).type_as(x)\n",
    "        return self.scale.to(x.device) * (x / norm)\n",
    "    \n",
    "\n",
    "def rope_scaling(freqs: torch.Tensor):\n",
    "    \"\"\"\n",
    "    From https://github.com/karpathy/nano-llama31/blob/master/llama31.py\n",
    "    \"\"\"\n",
    "    scale_factor = 8\n",
    "    low_freq_factor = 1\n",
    "    high_freq_factor = 4\n",
    "    old_context_len = 2048\n",
    "    low_freq_wavelen = old_context_len / low_freq_factor\n",
    "    high_freq_wavelen = old_context_len / high_freq_factor\n",
    "    new_freqs = []\n",
    "    for freq in freqs:\n",
    "        wavelen = 2 * math.pi / freq\n",
    "        if wavelen < high_freq_wavelen:\n",
    "            new_freqs.append(freq)\n",
    "        elif wavelen > low_freq_wavelen:\n",
    "            new_freqs.append(freq / scale_factor)\n",
    "        else:\n",
    "            assert low_freq_wavelen != high_freq_wavelen\n",
    "            smooth = (old_context_len / wavelen - low_freq_factor) / (\n",
    "                high_freq_factor - low_freq_factor\n",
    "            )\n",
    "            new_freqs.append((1 - smooth) * freq / scale_factor + smooth * freq)\n",
    "    return torch.tensor(new_freqs, dtype=freqs.dtype, device=freqs.device)\n",
    "\n",
    "def precompute_rope(dim, end, theta, use_scaled):\n",
    "    inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)) # theta\n",
    "    \n",
    "    if use_scaled:\n",
    "        inv_freq = rope_scaling(inv_freq)\n",
    "\n",
    "    position_ids = torch.arange(end, device=inv_freq.device, dtype=torch.float32) # en alguns llocs, seq_idx\n",
    "    \n",
    "    freqs = torch.outer(position_ids, inv_freq)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    freqs_cis_real = torch.stack([freqs_cis.real, freqs_cis.imag], dim=-1)\n",
    "    return freqs_cis_real\n",
    "\n",
    "def apply_rope(x, freqs_cis):\n",
    "    \"\"\"\n",
    "    Alt: Gemma implementation:\n",
    "    def apply_rotary_emb(x: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:\n",
    "    x_ = torch.view_as_complex(\n",
    "        torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1),\n",
    "                    dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis).type_as(x)\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2],\n",
    "                          -1).transpose(1, 2)\n",
    "    return x_out\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "    # xshaped is (bs, seqlen, n_heads, head_dim/2, 2), e.g. (4, 8, 32, 64, 2)\n",
    "\n",
    "    freqs_cis = freqs_cis.view(1, xshaped.size(1), 1, xshaped.size(3), 2)\n",
    "    # freqs_cis becomes (1, seqlen, 1, head_dim/2, 2), e.g. (1, 8, 1, 64, 2)\n",
    "    \n",
    "    x_out2 = torch.stack(\n",
    "        [\n",
    "            xshaped[..., 0] * freqs_cis[..., 0] - xshaped[..., 1] * freqs_cis[..., 1],\n",
    "            xshaped[..., 1] * freqs_cis[..., 0] + xshaped[..., 0] * freqs_cis[..., 1],\n",
    "        ],\n",
    "        -1,\n",
    "    )\n",
    "    # x_out2 at this point is (bs, seqlen, n_heads, head_dim/2, 2), e.g. (4, 8, 32, 64, 2)\n",
    "    x_out2 = x_out2.flatten(3)\n",
    "    # x_out2 is now (bs, seqlen, n_heads, head_dim), e.g. (4, 8, 32, 128)\n",
    "\n",
    "    return x_out2.type_as(x)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    class GeGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gate = x.chunk(2, dim=-1)\n",
    "        # Silu és el mateix que swift function\n",
    "        return F.gelu(gate) * x\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.w1   = nn.Linear(config.n_embd, config.intermediate_size, bias=False) # gate_proj\n",
    "        self.w2 = nn.Linear(config.intermediate_size, config.n_embd, bias=False) # down proj\n",
    "        self.w3 = nn.Linear(config.n_embd, config.intermediate_size, bias=False) # up proj\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.w2(F.gelu(self.w1(x), approximate=\"tanh\")* self.w3(x)) \n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.norm_1 = RMSNorm(config.n_embd, config.norm_eps) # input_layernorm\n",
    "        self.attn = CausalSelfAttentionGQA(config)\n",
    "        self.norm_2 = RMSNorm(config.n_embd, config.norm_eps) # post_attention_layernorm\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x, freq):\n",
    "        x = x + self.attn(self.norm_1(x), freq)\n",
    "        x = x + self.mlp(self.norm_2(x))\n",
    "        return x\n",
    "\n",
    "class Aloja(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd, padding_idx=3), #embed_tokens.weight\n",
    "            h = nn.ModuleList([Block(config) for i in range(config.n_layer)]),\n",
    "            norm_f =  RMSNorm(config.n_embd, config.norm_eps),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        self.transformer.wte.weight = self.lm_head.weight # Linkeddddd\n",
    "\n",
    "        self.freqs = precompute_rope(config.n_embd // config.n_head, config.max_seq_len * 2, config.rope_theta, config.use_scaled_rope)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None, start_pos:int = 0):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is smaller\"\n",
    "\n",
    "        x = self.transformer.wte(idx)\n",
    "        self.freqs = self.freqs.to(x.device)\n",
    "        freqs = self.freqs[:T]\n",
    "        mask = torch.full((T, T), float(\"-inf\"), device=idx.device)\n",
    "        mask = torch.triu(mask, diagonal=1)\n",
    "        mask = mask.type_as(x)\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x, freqs, mask)\n",
    "        x = self.transformer.norm_f(x)\n",
    "        logits = self.lm_head(x).float()\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                input=logits.transpose(1, 2),\n",
    "                target=targets,\n",
    "                reduction=\"mean\",\n",
    "                ignore_index=3,\n",
    "            )\n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self, learning_rate, weight_decay=0.0, betas=(0.9, 0.97), device_type='cuda'):\n",
    "        # start with all of the candidate parameters (that require grad)\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        if master_process:\n",
    "            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == \"cuda\"\n",
    "        if master_process:\n",
    "            print(f\"using fused AdamW: {use_fused}\")\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, eps=1e-8, fused=use_fused)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AlojaConfig:\n",
    "    block_size: int = 2048\n",
    "    vocab_size: int = 65536\n",
    "    n_layer: int = 32\n",
    "    n_head: int = 8\n",
    "    n_embd: int = 768\n",
    "    intermediate_size = 2048 # 3072\n",
    "    n_kv_heads: int = 4 # nombre de grups de query\n",
    "    norm_eps: int = 1e-5\n",
    "    rope_theta: float = 500000\n",
    "    use_scaled_rope: bool = False\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len:int = 2048\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, dataloader, checkpoint_path, device):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.config(checkpoint['config'])\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    step = checkpoint['step']\n",
    "    val_loss = checkpoint['val_loss']\n",
    "    print(f\"Checkpoint loaded from step {step} with val loss {val_loss}\")\n",
    "    return step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_19512\\3942902272.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"./src/model_00576.pt\", map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"./src/model_00576.pt\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Aloja(checkpoint['config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def visualize_attention(model, tokenizer, sentence, layer_num, head_num):\n",
    "    # Tokenize the input sentence\n",
    "    tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "\n",
    "    # Move input to the same device as the model\n",
    "    input_ids = input_ids.to(next(model.parameters()).device)\n",
    "\n",
    "    # Forward pass through the model with return_attention=True\n",
    "    _, attention_weights = model.transformer.h[layer_num].attn(model.transformer.wte(input_ids), model.freqs, return_attention=True)\n",
    "\n",
    "    # Select the attention weights for the specified head\n",
    "    attention_weights = attention_weights[0, head_num].detach().cpu().numpy()\n",
    "\n",
    "    # Plot the attention weights\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(attention_weights, cmap=\"viridis\", xticklabels=sentence.split(), yticklabels=sentence.split())\n",
    "    plt.title(f\"Attention Weights - Layer {layer_num + 1}, Head {head_num + 1}\")\n",
    "    plt.xlabel(\"Key Position\")\n",
    "    plt.ylabel(\"Query Position\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_attention(model, enc, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.012831157073378563,\n",
       " 0.0065925405360758305,\n",
       " -0.007648141589015722,\n",
       " 0.019636638462543488,\n",
       " 0.02578912302851677,\n",
       " 0.015744853764772415,\n",
       " -0.023119593039155006,\n",
       " -0.01135143730789423,\n",
       " -0.011666991747915745,\n",
       " 0.019317064434289932,\n",
       " 0.020809516310691833,\n",
       " 0.00041685145697556436,\n",
       " -0.010931160300970078,\n",
       " 0.015505609102547169,\n",
       " -0.008102518506348133,\n",
       " 0.019798098132014275,\n",
       " 0.06340011954307556,\n",
       " -0.009394454769790173,\n",
       " 0.005274095572531223,\n",
       " -0.015478570014238358,\n",
       " 0.007816036231815815,\n",
       " -0.012796544469892979,\n",
       " 0.024425633251667023,\n",
       " -0.013300667516887188,\n",
       " -0.016700919717550278,\n",
       " 0.014346715994179249,\n",
       " -0.006231224630028009,\n",
       " 0.00986629631370306,\n",
       " -0.004729934502393007,\n",
       " 0.015897955745458603,\n",
       " -0.000613517127931118,\n",
       " -0.010332741774618626,\n",
       " 0.024980813264846802,\n",
       " -0.010868091136217117,\n",
       " -0.0029572120402008295,\n",
       " 0.016337065026164055,\n",
       " 0.004842178430408239,\n",
       " -0.004348214250057936,\n",
       " 0.013645288534462452,\n",
       " -0.0030171643011271954,\n",
       " -0.009156519547104836,\n",
       " -0.016477586701512337,\n",
       " 0.009412320330739021,\n",
       " -0.0007997163920663297,\n",
       " -0.018652595579624176,\n",
       " 0.004600841552019119,\n",
       " -0.010532121174037457,\n",
       " -0.010413878597319126,\n",
       " 0.007050860207527876,\n",
       " -0.005648233462125063,\n",
       " 0.004082530736923218,\n",
       " -0.016808245331048965,\n",
       " -0.0004966397536918521,\n",
       " -0.004458146169781685,\n",
       " 0.0016152294119819999,\n",
       " 0.000647307257167995,\n",
       " 0.006173634435981512,\n",
       " 0.004104625899344683,\n",
       " 0.022667791694402695,\n",
       " 0.006356942001730204,\n",
       " 0.04030723124742508,\n",
       " 0.00584777956828475,\n",
       " -0.003047741949558258,\n",
       " 0.006783561781048775,\n",
       " 0.016528666019439697,\n",
       " 0.024308785796165466,\n",
       " 0.017484763637185097,\n",
       " 0.011263697408139706,\n",
       " 6.664924148935825e-05,\n",
       " 0.016946421936154366,\n",
       " -0.031393036246299744,\n",
       " 0.011077919974923134,\n",
       " -0.022788049653172493,\n",
       " -0.014976348727941513,\n",
       " 0.00368715962395072,\n",
       " -0.011079895310103893,\n",
       " -0.013387991115450859,\n",
       " -0.016673289239406586,\n",
       " 0.009265567176043987,\n",
       " -0.0008935185614973307,\n",
       " 0.01442441251128912,\n",
       " 0.0003500971361063421,\n",
       " -0.0057489932514727116,\n",
       " 0.002760038711130619,\n",
       " 0.015271485783159733,\n",
       " 0.018042441457509995,\n",
       " -0.011617433279752731,\n",
       " 0.02472064457833767,\n",
       " 0.003142019733786583,\n",
       " -0.023041700944304466,\n",
       " 0.008334185928106308,\n",
       " 0.008066529408097267,\n",
       " -0.00022573911701329052,\n",
       " 0.0015818326501175761,\n",
       " 0.006465392652899027,\n",
       " -0.00667992839589715,\n",
       " 0.01811283454298973,\n",
       " 0.01692746765911579,\n",
       " -0.021320046856999397,\n",
       " 0.011051928624510765,\n",
       " 0.0264233760535717,\n",
       " -0.0028069450054317713,\n",
       " -0.0008387905545532703,\n",
       " -0.021697942167520523,\n",
       " -0.011267761699855328,\n",
       " 0.0007712090155109763,\n",
       " -0.01019552443176508,\n",
       " -0.0077856313437223434,\n",
       " 0.00297879078425467,\n",
       " 0.01745324209332466,\n",
       " -0.015239457599818707,\n",
       " -0.007979350164532661,\n",
       " -0.022416647523641586,\n",
       " -0.0020492193289101124,\n",
       " 0.013042664155364037,\n",
       " -0.02799096144735813,\n",
       " -0.009624543599784374,\n",
       " -0.003148610470816493,\n",
       " -0.004171411041170359,\n",
       " 0.0011516633676365018,\n",
       " -0.02158810943365097,\n",
       " -0.01812422089278698,\n",
       " 0.008465936407446861,\n",
       " 0.0007672907086089253,\n",
       " 0.00843103788793087,\n",
       " -0.024939607828855515,\n",
       " -0.01395128108561039,\n",
       " 0.010476069524884224,\n",
       " -0.00676822429522872,\n",
       " -0.03655108064413071,\n",
       " 0.047037579119205475,\n",
       " 0.017850646749138832,\n",
       " 0.007939860224723816,\n",
       " -0.014323168434202671,\n",
       " 0.015130195766687393,\n",
       " 0.014508841559290886,\n",
       " -0.025300409644842148,\n",
       " 0.012930894270539284,\n",
       " -0.007997002452611923,\n",
       " 0.014476488344371319,\n",
       " -0.012330293655395508,\n",
       " 0.021978726610541344,\n",
       " -0.009830409660935402,\n",
       " -0.009348603896796703,\n",
       " 0.01015760749578476,\n",
       " -0.0102238142862916,\n",
       " -0.0032961787655949593,\n",
       " -0.0006575068109668791,\n",
       " 0.014437529258430004,\n",
       " -0.016758739948272705,\n",
       " -0.0485118106007576,\n",
       " -0.02838306501507759,\n",
       " 0.017977742478251457,\n",
       " -0.02092849276959896,\n",
       " 0.009817622601985931,\n",
       " -0.024759763851761818,\n",
       " 0.003957877401262522,\n",
       " 0.00028175426996313035,\n",
       " -0.0016354674007743597,\n",
       " 0.013557282276451588,\n",
       " 0.0011028206208720803,\n",
       " -0.0002487186575308442,\n",
       " 0.0008141805301420391,\n",
       " -0.013798516243696213,\n",
       " 0.0002695510338526219,\n",
       " -0.008237375877797604,\n",
       " -0.019148895516991615,\n",
       " 0.012462733313441277,\n",
       " 0.0074999150820076466,\n",
       " -0.004783464130014181,\n",
       " -0.007671001832932234,\n",
       " 0.0035176551900804043,\n",
       " -0.004994282498955727,\n",
       " 0.0007579855737276375,\n",
       " 0.00452802237123251,\n",
       " 0.04349903762340546,\n",
       " 0.015332394279539585,\n",
       " -0.00970077607780695,\n",
       " 0.009206634014844894,\n",
       " -0.014391480013728142,\n",
       " 0.004076637327671051,\n",
       " -0.025961678475141525,\n",
       " 0.01477639190852642,\n",
       " -0.0049008517526090145,\n",
       " -0.020668184384703636,\n",
       " -0.0006717224605381489,\n",
       " 0.02047291211783886,\n",
       " -0.0029267275240272284,\n",
       " 0.023639580234885216,\n",
       " 0.02362353540956974,\n",
       " -0.01308374386280775,\n",
       " 0.011637046001851559,\n",
       " 0.02181224711239338,\n",
       " -0.006120903417468071,\n",
       " 0.012739831581711769,\n",
       " 0.006984082516282797,\n",
       " 0.003542055608704686,\n",
       " -0.018501518294215202,\n",
       " 0.0061026341281831264,\n",
       " -0.008832060731947422,\n",
       " 0.004321437329053879,\n",
       " -0.015682753175497055,\n",
       " -0.017252858728170395,\n",
       " 0.01316074002534151,\n",
       " 0.0163706224411726,\n",
       " -0.007065165787935257,\n",
       " -0.024580666795372963,\n",
       " -0.018443070352077484,\n",
       " 0.008590633049607277,\n",
       " -0.0023088082671165466,\n",
       " -0.008864058181643486,\n",
       " 0.0033864188008010387,\n",
       " 0.004216598812490702,\n",
       " 0.01494970079511404,\n",
       " 0.007152438163757324,\n",
       " 0.013384329155087471,\n",
       " 0.004948707297444344,\n",
       " 0.02206096425652504,\n",
       " -0.005175631958991289,\n",
       " 0.019410734996199608,\n",
       " -0.02395034022629261,\n",
       " -0.019552504643797874,\n",
       " -0.022833911702036858,\n",
       " 0.013409361243247986,\n",
       " -0.0230559054762125,\n",
       " 0.019251802936196327,\n",
       " -0.016055943444371223,\n",
       " 0.022744938731193542,\n",
       " 0.0007256193784996867,\n",
       " -0.012180187739431858,\n",
       " 0.013508782722055912,\n",
       " 0.013997195288538933,\n",
       " 0.013488241471350193,\n",
       " 0.0027644538786262274,\n",
       " 0.0014165538595989347,\n",
       " 0.026230979710817337,\n",
       " -0.02736898511648178,\n",
       " -0.007872343063354492,\n",
       " -0.011879161931574345,\n",
       " -0.003489538561552763,\n",
       " 0.020901963114738464,\n",
       " 0.01235184445977211,\n",
       " -0.048702359199523926,\n",
       " 0.0010114778997376561,\n",
       " 0.020551029592752457,\n",
       " 0.009799308143556118,\n",
       " 0.004738489165902138,\n",
       " 0.006904478184878826,\n",
       " 0.012758927419781685,\n",
       " -0.006033871788531542,\n",
       " -0.0009564565261825919,\n",
       " -0.0035349191166460514,\n",
       " -0.02254924736917019,\n",
       " 0.0074816481210291386,\n",
       " 0.01821921020746231,\n",
       " 0.017277436330914497,\n",
       " -0.04128270596265793,\n",
       " 0.0009423792362213135,\n",
       " 0.0107102170586586,\n",
       " 0.030184853821992874,\n",
       " -0.02827600948512554,\n",
       " -0.007351752370595932,\n",
       " -0.0006193916779011488,\n",
       " 0.017559025436639786,\n",
       " -0.023293012753129005,\n",
       " 0.0065164389088749886,\n",
       " -0.012308008037507534,\n",
       " -0.005890443921089172,\n",
       " 0.001578410156071186,\n",
       " 0.013586186803877354,\n",
       " 0.008527392521500587,\n",
       " -0.0008072248310782015,\n",
       " -0.0004719408170785755,\n",
       " -0.015454430133104324,\n",
       " -0.012120232917368412,\n",
       " -0.006115483585745096,\n",
       " 0.012968901544809341,\n",
       " 0.01238077599555254,\n",
       " 0.005467805080115795,\n",
       " -0.03147672116756439,\n",
       " -0.005335439462214708,\n",
       " 0.00871576089411974,\n",
       " 0.00798727385699749,\n",
       " -0.006596621125936508,\n",
       " 0.004313919227570295,\n",
       " -0.005710686556994915,\n",
       " 0.01029735617339611,\n",
       " -1.77016991074197e-05,\n",
       " 0.01060587540268898,\n",
       " -0.017461949959397316,\n",
       " 0.01979859545826912,\n",
       " 0.005112771410495043,\n",
       " 0.007283447775989771,\n",
       " -0.0118447570130229,\n",
       " -0.008214355446398258,\n",
       " 0.028354773297905922,\n",
       " 0.008827085606753826,\n",
       " -9.95821101241745e-05,\n",
       " 0.006024250760674477,\n",
       " -0.020796604454517365,\n",
       " 0.02633444406092167,\n",
       " -0.007430272176861763,\n",
       " 0.013735258020460606,\n",
       " -0.010614238679409027,\n",
       " 0.029173128306865692,\n",
       " -0.005393059924244881,\n",
       " 0.0064488849602639675,\n",
       " -0.02233419194817543,\n",
       " -0.017241667956113815,\n",
       " -0.005817520897835493,\n",
       " -0.013504833914339542,\n",
       " -0.019959405064582825,\n",
       " 0.014063810929656029,\n",
       " 0.012240346521139145,\n",
       " -0.003103028517216444,\n",
       " -0.0109000438824296,\n",
       " -0.010181261226534843,\n",
       " 0.007956483401358128,\n",
       " 0.002025829628109932,\n",
       " 0.005139142274856567,\n",
       " 0.02212192676961422,\n",
       " 0.005662522744387388,\n",
       " 0.026422424241900444,\n",
       " 0.017275085672736168,\n",
       " 0.019631793722510338,\n",
       " -0.007475058548152447,\n",
       " -0.006366534624248743,\n",
       " 0.005444930400699377,\n",
       " -0.011072556488215923,\n",
       " -9.927780774887651e-05,\n",
       " -0.0020678541623055935,\n",
       " -0.009102348238229752,\n",
       " 0.0094416793435812,\n",
       " 0.00045739905908703804,\n",
       " 0.027104416862130165,\n",
       " -0.024907821789383888,\n",
       " -0.008811465464532375,\n",
       " -0.014721355400979519,\n",
       " -0.001557676587253809,\n",
       " -0.012181918136775494,\n",
       " 0.011662380769848824,\n",
       " 0.02427223138511181,\n",
       " -0.015465491451323032,\n",
       " 0.005154658108949661,\n",
       " -0.02031407505273819,\n",
       " -0.013825592584908009,\n",
       " -0.017337629571557045,\n",
       " -0.024986453354358673,\n",
       " -0.004954180680215359,\n",
       " 0.012319779954850674,\n",
       " 0.03229817748069763,\n",
       " -0.01833282969892025,\n",
       " 0.0008424537954851985,\n",
       " 0.021724548190832138,\n",
       " 0.0036581510212272406,\n",
       " 0.018703540787100792,\n",
       " -0.0060172914527356625,\n",
       " 0.000159389222972095,\n",
       " 0.007098381873220205,\n",
       " -0.022067841142416,\n",
       " 0.034760091453790665,\n",
       " 0.00040923195774666965,\n",
       " -0.019829250872135162,\n",
       " 0.014742798171937466,\n",
       " -0.013203989714384079,\n",
       " 0.022237073630094528,\n",
       " 0.032867997884750366,\n",
       " -0.008618258871138096,\n",
       " -0.026777897030115128,\n",
       " 0.012065309099853039,\n",
       " 0.007169797550886869,\n",
       " 0.0061761727556586266,\n",
       " 0.012368918396532536,\n",
       " 0.012723160907626152,\n",
       " -0.006728478707373142,\n",
       " 0.015520039014518261,\n",
       " 0.04008273035287857,\n",
       " 0.005253381095826626,\n",
       " 0.008111251518130302,\n",
       " -0.020881786942481995,\n",
       " 0.011335758492350578,\n",
       " -0.0031061465851962566,\n",
       " 0.00692169601097703,\n",
       " 0.026733845472335815,\n",
       " -0.03484254330396652,\n",
       " -0.01409514807164669,\n",
       " -0.01406823843717575,\n",
       " 0.007935181260108948,\n",
       " 0.006663098931312561,\n",
       " -0.004031225573271513,\n",
       " -0.02655622735619545,\n",
       " 0.020213723182678223,\n",
       " 0.011863871477544308,\n",
       " -0.004830085672438145,\n",
       " 0.002944775391370058,\n",
       " 0.011213383637368679,\n",
       " -0.002941973740234971,\n",
       " -0.008038690313696861,\n",
       " 0.021029828116297722,\n",
       " 0.018896305933594704,\n",
       " 0.030987398698925972,\n",
       " 0.01189498882740736,\n",
       " -0.002917120698839426,\n",
       " 0.020556503906846046,\n",
       " 0.002727856393903494,\n",
       " -0.023244943469762802,\n",
       " -0.003319743787869811,\n",
       " -0.0018783904379233718,\n",
       " 0.0049393437802791595,\n",
       " -0.0020854005124419928,\n",
       " -0.0013982558157294989,\n",
       " -0.00475369393825531,\n",
       " -0.03265581279993057,\n",
       " 0.00706533994525671,\n",
       " -0.01353193074464798,\n",
       " -0.009961292147636414,\n",
       " -0.01744680665433407,\n",
       " -0.01613379828631878,\n",
       " 0.007812274619936943,\n",
       " -0.02503252401947975,\n",
       " -0.01914054900407791,\n",
       " -0.005272955168038607,\n",
       " -0.017276009544730186,\n",
       " 0.00928579643368721,\n",
       " -0.0033328705467283726,\n",
       " 0.0002965392777696252,\n",
       " -0.020972222089767456,\n",
       " 0.00735278706997633,\n",
       " -0.002027292735874653,\n",
       " 0.015063246712088585,\n",
       " 0.010649863630533218,\n",
       " 0.02287294901907444,\n",
       " -0.007240500301122665,\n",
       " -0.030976777896285057,\n",
       " 0.005407997407019138,\n",
       " 0.030253905802965164,\n",
       " -0.005628770217299461,\n",
       " -0.008988519199192524,\n",
       " 0.002808681456372142,\n",
       " 0.013200728222727776,\n",
       " -0.01842736266553402,\n",
       " -0.010530144907534122,\n",
       " 0.00726214749738574,\n",
       " 0.014138532802462578,\n",
       " -0.016866575926542282,\n",
       " 0.006291468162089586,\n",
       " -0.015105966478586197,\n",
       " -0.05617127940058708,\n",
       " 0.00020584178855642676,\n",
       " -0.03761494904756546,\n",
       " 0.007236470002681017,\n",
       " 0.03945989906787872,\n",
       " -0.01738610491156578,\n",
       " 0.0058279866352677345,\n",
       " 0.008750797249376774,\n",
       " 0.014223078265786171,\n",
       " 0.005207831040024757,\n",
       " -0.030772706493735313,\n",
       " -0.01421050913631916,\n",
       " -0.033885061740875244,\n",
       " -0.00610749889165163,\n",
       " -0.007442481350153685,\n",
       " -0.0019211850594729185,\n",
       " -0.025375952944159508,\n",
       " 0.006842891685664654,\n",
       " -0.016862519085407257,\n",
       " 0.0012358501553535461,\n",
       " -0.00735060777515173,\n",
       " 0.03358284756541252,\n",
       " 0.026072710752487183,\n",
       " 0.003685307689011097,\n",
       " 0.029647374525666237,\n",
       " -0.006552610080689192,\n",
       " 0.01577308587729931,\n",
       " 0.00937501061707735,\n",
       " -0.027696911245584488,\n",
       " -0.006223219912499189,\n",
       " 0.0034556433092802763,\n",
       " 0.01544841006398201,\n",
       " 0.015153484418988228,\n",
       " -0.01285488810390234,\n",
       " -0.00872812606394291,\n",
       " 0.017278974875807762,\n",
       " 0.00866072066128254,\n",
       " 0.006022702902555466,\n",
       " -0.0222243033349514,\n",
       " 0.0023618098348379135,\n",
       " -0.010650837793946266,\n",
       " -0.027309482917189598,\n",
       " -0.01588563621044159,\n",
       " -0.02401566319167614,\n",
       " -0.021633939817547798,\n",
       " -0.015940871089696884,\n",
       " 0.007657649926841259,\n",
       " -0.0016788211651146412,\n",
       " -0.003025488695129752,\n",
       " 0.0013793722027912736,\n",
       " 0.003262876532971859,\n",
       " -0.012367351911962032,\n",
       " 0.012910289689898491,\n",
       " -0.008217797614634037,\n",
       " -0.0027700832579284906,\n",
       " 0.020563166588544846,\n",
       " 0.0009421178256161511,\n",
       " 0.013475432991981506,\n",
       " 0.010285865515470505,\n",
       " -0.019218645989894867,\n",
       " 0.01860211417078972,\n",
       " -0.02119777910411358,\n",
       " -0.003725208807736635,\n",
       " -0.0201712679117918,\n",
       " 0.01436990313231945,\n",
       " -0.011929133906960487,\n",
       " 0.011081728152930737,\n",
       " -0.02338489703834057,\n",
       " -0.012487711384892464,\n",
       " -0.010542202740907669,\n",
       " -0.02609582431614399,\n",
       " -0.00839481595903635,\n",
       " 0.016602518036961555,\n",
       " 0.035164061933755875,\n",
       " -0.010684389621019363,\n",
       " 0.010931952856481075,\n",
       " -0.00289582135155797,\n",
       " 0.01410262007266283,\n",
       " 0.011363326571881771,\n",
       " 0.010657982900738716,\n",
       " 0.0014978987164795399,\n",
       " -0.005295243114233017,\n",
       " -0.0025783111341297626,\n",
       " -0.01758650504052639,\n",
       " 0.0063836658373475075,\n",
       " -0.012759856879711151,\n",
       " -0.0013823818881064653,\n",
       " 0.008961471728980541,\n",
       " 0.00045579555444419384,\n",
       " -0.020907048135995865,\n",
       " -0.008022657595574856,\n",
       " -0.01608210802078247,\n",
       " 0.015819039195775986,\n",
       " 0.017052441835403442,\n",
       " 0.00015019938291516155,\n",
       " 0.009495178237557411,\n",
       " 0.00012992043048143387,\n",
       " 0.02700514905154705,\n",
       " -0.0012389845214784145,\n",
       " 0.0145180094987154,\n",
       " -0.009879673831164837,\n",
       " 0.00012378045357763767,\n",
       " -0.00905174296349287,\n",
       " -0.006325521040707827,\n",
       " 0.012462193146348,\n",
       " -0.00695654796436429,\n",
       " -0.02507910691201687,\n",
       " 0.012992276810109615,\n",
       " -0.0033288297709077597,\n",
       " 0.033488135784864426,\n",
       " 0.012452887371182442,\n",
       " 0.00025309238117188215,\n",
       " 0.008101972751319408,\n",
       " -0.002305753994733095,\n",
       " 0.0024013156071305275,\n",
       " 0.025679050013422966,\n",
       " 0.01660933904349804,\n",
       " 0.010435190051794052,\n",
       " -0.010572521947324276,\n",
       " -0.0024966062046587467,\n",
       " -0.007658008486032486,\n",
       " -0.03155823424458504,\n",
       " -0.002388037508353591,\n",
       " -0.016137387603521347,\n",
       " 0.02564726211130619,\n",
       " -0.015597453340888023,\n",
       " -6.934827251825482e-05,\n",
       " -0.006439703516662121,\n",
       " -0.0036518413107842207,\n",
       " -0.013942037709057331,\n",
       " 0.006199339870363474,\n",
       " -0.005638880655169487,\n",
       " 0.010175056755542755,\n",
       " -0.009584402665495872,\n",
       " 0.0145548265427351,\n",
       " -0.014599516987800598,\n",
       " -0.019525494426488876,\n",
       " -0.0123774204403162,\n",
       " -0.0183486957103014,\n",
       " -0.015219547785818577,\n",
       " 0.005748725961893797,\n",
       " 0.014635296538472176,\n",
       " -0.006330947391688824,\n",
       " 0.004188487306237221,\n",
       " 0.006580602377653122,\n",
       " -0.002081411425024271,\n",
       " 0.013553909957408905,\n",
       " -0.007145203184336424,\n",
       " -0.0160627793520689,\n",
       " 0.019510893151164055,\n",
       " -0.0018747916910797358,\n",
       " -0.02318730764091015,\n",
       " -0.0052321976982057095,\n",
       " -0.0018406652379781008,\n",
       " -0.01248590461909771,\n",
       " -0.009042823687195778,\n",
       " -0.030186031013727188,\n",
       " -0.006422982085496187,\n",
       " 0.008657262660562992,\n",
       " 0.042880043387413025,\n",
       " 0.014224410988390446,\n",
       " -0.031076066195964813,\n",
       " 0.0006998251192271709,\n",
       " -0.0014577663969248533,\n",
       " -0.01299271173775196,\n",
       " 0.00378371006809175,\n",
       " 0.01604510471224785,\n",
       " -0.022238783538341522,\n",
       " -0.006447992753237486,\n",
       " 0.018522704020142555,\n",
       " 0.013017416931688786,\n",
       " -0.004138637334108353,\n",
       " 0.02079993672668934,\n",
       " 0.01571137271821499,\n",
       " 0.005614003166556358,\n",
       " 0.009622935205698013,\n",
       " 0.008106022141873837,\n",
       " -0.003921612165868282,\n",
       " 0.011898302473127842,\n",
       " 0.016186809167265892,\n",
       " -0.015952248126268387,\n",
       " 0.006847800221294165,\n",
       " -0.017121879383921623,\n",
       " 0.00604218477383256,\n",
       " 0.016478003934025764,\n",
       " -0.007022615987807512,\n",
       " -0.030229121446609497,\n",
       " -0.012967310845851898,\n",
       " -0.035226382315158844,\n",
       " 0.016307422891259193,\n",
       " 0.017449159175157547,\n",
       " 0.003197093727067113,\n",
       " -0.04458009451627731,\n",
       " 0.01399555616080761,\n",
       " 0.024549873545765877,\n",
       " -0.0020222121383994818,\n",
       " -0.03696459159255028,\n",
       " 0.004409115295857191,\n",
       " -0.0062658702954649925,\n",
       " 0.01149588543921709,\n",
       " -0.011204239912331104,\n",
       " -0.003779058111831546,\n",
       " 0.017548471689224243,\n",
       " -0.01769890822470188,\n",
       " 0.010882722213864326,\n",
       " -0.0034274118952453136,\n",
       " -0.005032787099480629,\n",
       " 0.0030582875479012728,\n",
       " 0.002977818949148059,\n",
       " 0.0017949395114555955,\n",
       " 0.007327989675104618,\n",
       " -0.006934690289199352,\n",
       " 0.0015782564878463745,\n",
       " -0.020175311714410782,\n",
       " 0.0056983609683811665,\n",
       " 0.01611592434346676,\n",
       " -0.004819717723876238,\n",
       " 0.03432197868824005,\n",
       " 0.0001380805333610624,\n",
       " 4.2975520045729354e-05,\n",
       " -0.010556700639426708,\n",
       " -0.005728756543248892,\n",
       " 0.009124960750341415,\n",
       " 0.002998590236529708,\n",
       " -0.0053921048529446125,\n",
       " 0.026803141459822655,\n",
       " 0.024642635136842728,\n",
       " -0.015792973339557648,\n",
       " 0.012333686463534832,\n",
       " 0.014606638811528683,\n",
       " 0.008968927897512913,\n",
       " 0.002893883967772126,\n",
       " -0.003882419317960739,\n",
       " -0.005554589442908764,\n",
       " -0.003023516619578004,\n",
       " -0.0291304849088192,\n",
       " -0.011007685214281082,\n",
       " -0.0111426105722785,\n",
       " -0.0244620218873024,\n",
       " -0.0179037693887949,\n",
       " -0.004046427085995674,\n",
       " 0.013746307231485844,\n",
       " -0.0012138844467699528,\n",
       " 0.010501546785235405,\n",
       " -0.010751565918326378,\n",
       " -0.0262621883302927,\n",
       " 0.005553783383220434,\n",
       " 0.008996838703751564,\n",
       " 0.0008222345495596528,\n",
       " 0.016705233603715897,\n",
       " -0.00895187072455883,\n",
       " -0.0013860100880265236,\n",
       " -0.013055670075118542,\n",
       " 0.019211478531360626,\n",
       " 0.0018789133755490184,\n",
       " 0.01118894387036562,\n",
       " -0.000614235526882112,\n",
       " -0.021604100242257118,\n",
       " -0.0022795135155320168,\n",
       " -0.013853599317371845,\n",
       " 0.003537957789376378,\n",
       " -0.020458631217479706,\n",
       " -0.005694213323295116,\n",
       " 0.011240008287131786,\n",
       " -0.0003268862492404878,\n",
       " -0.015334347262978554,\n",
       " 0.012911704368889332,\n",
       " 0.0070726326666772366,\n",
       " -0.002645248780027032,\n",
       " -0.0038885632529854774,\n",
       " 0.0038937400095164776,\n",
       " -0.010482069104909897,\n",
       " 0.017545290291309357,\n",
       " -0.005503559019416571,\n",
       " -0.030839186161756516,\n",
       " 0.004024396650493145,\n",
       " -0.007272220216691494,\n",
       " 0.0002634921111166477,\n",
       " -0.018834389746189117,\n",
       " 0.0033223824575543404,\n",
       " 0.005151010118424892,\n",
       " 0.0008516621310263872,\n",
       " 0.013521681539714336,\n",
       " -0.0023151347413659096,\n",
       " -0.0008585957693867385,\n",
       " -0.012648317031562328,\n",
       " -0.016390128061175346,\n",
       " -0.008503234013915062,\n",
       " 0.008133554831147194,\n",
       " 0.0010335403494536877,\n",
       " -0.001215416588820517,\n",
       " -0.01098153181374073,\n",
       " -0.02631089836359024,\n",
       " 0.004888833034783602,\n",
       " 0.009268652647733688,\n",
       " 0.028255902230739594,\n",
       " -0.012524601072072983,\n",
       " -0.011152197606861591,\n",
       " -0.0006034011603333056,\n",
       " -0.00023681498714722693,\n",
       " 0.028582917526364326,\n",
       " 0.00910672266036272,\n",
       " 0.019468767568469048,\n",
       " 0.005595740396529436,\n",
       " 0.005955451633781195,\n",
       " 0.0014561171410605311,\n",
       " -0.009575074538588524,\n",
       " 0.021983444690704346,\n",
       " -0.005197111051529646,\n",
       " -0.016078807413578033,\n",
       " -0.02060866728425026,\n",
       " -0.012710769660770893,\n",
       " 0.031143026426434517,\n",
       " -0.020769154652953148,\n",
       " -0.020356683060526848,\n",
       " 0.033224888145923615,\n",
       " 0.012083209119737148,\n",
       " -0.002275628736242652,\n",
       " -0.018309583887457848,\n",
       " -0.01339038647711277,\n",
       " 0.007063995115458965]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer.wte.weight',\n",
       " 'transformer.h.0.norm_1.scale',\n",
       " 'transformer.h.0.attn.c_attn.weight',\n",
       " 'transformer.h.0.attn.o_proj.weight']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.state_dict())[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Inconsistent shape between the condition and the input (got (768, 1) and (768,))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransformer.h.0.attn.c_attn.weight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mviridis\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention Weights\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey Position\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\seaborn\\matrix.py:446\u001b[0m, in \u001b[0;36mheatmap\u001b[1;34m(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Plot rectangular data as a color-encoded matrix.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03mThis is an Axes-level function and will draw the heatmap into the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    443\u001b[0m \n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# Initialize the plotter object\u001b[39;00m\n\u001b[1;32m--> 446\u001b[0m plotter \u001b[38;5;241m=\u001b[39m \u001b[43m_HeatMapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobust\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mannot_kws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbar_kws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxticklabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m                      \u001b[49m\u001b[43myticklabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Add the pcolormesh kwargs here\u001b[39;00m\n\u001b[0;32m    451\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinewidths\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m linewidths\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\seaborn\\matrix.py:115\u001b[0m, in \u001b[0;36m_HeatMapper.__init__\u001b[1;34m(self, data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Validate the mask and convert to DataFrame\u001b[39;00m\n\u001b[0;32m    113\u001b[0m mask \u001b[38;5;241m=\u001b[39m _matrix_mask(data, mask)\n\u001b[1;32m--> 115\u001b[0m plot_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_where\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Get good names for the rows and columns\u001b[39;00m\n\u001b[0;32m    118\u001b[0m xtickevery \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\ma\\core.py:1933\u001b[0m, in \u001b[0;36mmasked_where\u001b[1;34m(condition, a, copy)\u001b[0m\n\u001b[0;32m   1931\u001b[0m (cshape, ashape) \u001b[38;5;241m=\u001b[39m (cond\u001b[38;5;241m.\u001b[39mshape, a\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m   1932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cshape \u001b[38;5;129;01mand\u001b[39;00m cshape \u001b[38;5;241m!=\u001b[39m ashape:\n\u001b[1;32m-> 1933\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInconsistent shape between the condition and the input\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1934\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (cshape, ashape))\n\u001b[0;32m   1935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_mask\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   1936\u001b[0m     cond \u001b[38;5;241m=\u001b[39m mask_or(cond, a\u001b[38;5;241m.\u001b[39m_mask)\n",
      "\u001b[1;31mIndexError\u001b[0m: Inconsistent shape between the condition and the input (got (768, 1) and (768,))"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(model.state_dict()[\"transformer.h.0.attn.c_attn.weight\"][0].tolist(), cmap=\"viridis\")\n",
    "plt.title(f\"Attention Weights\")\n",
    "plt.xlabel(\"Key Position\")\n",
    "plt.ylabel(\"Query Position\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2831e-02,  6.5925e-03, -7.6481e-03,  1.9637e-02,  2.5789e-02,\n",
       "         1.5745e-02, -2.3120e-02, -1.1351e-02, -1.1667e-02,  1.9317e-02,\n",
       "         2.0810e-02,  4.1685e-04, -1.0931e-02,  1.5506e-02, -8.1025e-03,\n",
       "         1.9798e-02,  6.3400e-02, -9.3945e-03,  5.2741e-03, -1.5479e-02,\n",
       "         7.8160e-03, -1.2797e-02,  2.4426e-02, -1.3301e-02, -1.6701e-02,\n",
       "         1.4347e-02, -6.2312e-03,  9.8663e-03, -4.7299e-03,  1.5898e-02,\n",
       "        -6.1352e-04, -1.0333e-02,  2.4981e-02, -1.0868e-02, -2.9572e-03,\n",
       "         1.6337e-02,  4.8422e-03, -4.3482e-03,  1.3645e-02, -3.0172e-03,\n",
       "        -9.1565e-03, -1.6478e-02,  9.4123e-03, -7.9972e-04, -1.8653e-02,\n",
       "         4.6008e-03, -1.0532e-02, -1.0414e-02,  7.0509e-03, -5.6482e-03,\n",
       "         4.0825e-03, -1.6808e-02, -4.9664e-04, -4.4581e-03,  1.6152e-03,\n",
       "         6.4731e-04,  6.1736e-03,  4.1046e-03,  2.2668e-02,  6.3569e-03,\n",
       "         4.0307e-02,  5.8478e-03, -3.0477e-03,  6.7836e-03,  1.6529e-02,\n",
       "         2.4309e-02,  1.7485e-02,  1.1264e-02,  6.6649e-05,  1.6946e-02,\n",
       "        -3.1393e-02,  1.1078e-02, -2.2788e-02, -1.4976e-02,  3.6872e-03,\n",
       "        -1.1080e-02, -1.3388e-02, -1.6673e-02,  9.2656e-03, -8.9352e-04,\n",
       "         1.4424e-02,  3.5010e-04, -5.7490e-03,  2.7600e-03,  1.5271e-02,\n",
       "         1.8042e-02, -1.1617e-02,  2.4721e-02,  3.1420e-03, -2.3042e-02,\n",
       "         8.3342e-03,  8.0665e-03, -2.2574e-04,  1.5818e-03,  6.4654e-03,\n",
       "        -6.6799e-03,  1.8113e-02,  1.6927e-02, -2.1320e-02,  1.1052e-02,\n",
       "         2.6423e-02, -2.8069e-03, -8.3879e-04, -2.1698e-02, -1.1268e-02,\n",
       "         7.7121e-04, -1.0196e-02, -7.7856e-03,  2.9788e-03,  1.7453e-02,\n",
       "        -1.5239e-02, -7.9794e-03, -2.2417e-02, -2.0492e-03,  1.3043e-02,\n",
       "        -2.7991e-02, -9.6245e-03, -3.1486e-03, -4.1714e-03,  1.1517e-03,\n",
       "        -2.1588e-02, -1.8124e-02,  8.4659e-03,  7.6729e-04,  8.4310e-03,\n",
       "        -2.4940e-02, -1.3951e-02,  1.0476e-02, -6.7682e-03, -3.6551e-02,\n",
       "         4.7038e-02,  1.7851e-02,  7.9399e-03, -1.4323e-02,  1.5130e-02,\n",
       "         1.4509e-02, -2.5300e-02,  1.2931e-02, -7.9970e-03,  1.4476e-02,\n",
       "        -1.2330e-02,  2.1979e-02, -9.8304e-03, -9.3486e-03,  1.0158e-02,\n",
       "        -1.0224e-02, -3.2962e-03, -6.5751e-04,  1.4438e-02, -1.6759e-02,\n",
       "        -4.8512e-02, -2.8383e-02,  1.7978e-02, -2.0928e-02,  9.8176e-03,\n",
       "        -2.4760e-02,  3.9579e-03,  2.8175e-04, -1.6355e-03,  1.3557e-02,\n",
       "         1.1028e-03, -2.4872e-04,  8.1418e-04, -1.3799e-02,  2.6955e-04,\n",
       "        -8.2374e-03, -1.9149e-02,  1.2463e-02,  7.4999e-03, -4.7835e-03,\n",
       "        -7.6710e-03,  3.5177e-03, -4.9943e-03,  7.5799e-04,  4.5280e-03,\n",
       "         4.3499e-02,  1.5332e-02, -9.7008e-03,  9.2066e-03, -1.4391e-02,\n",
       "         4.0766e-03, -2.5962e-02,  1.4776e-02, -4.9009e-03, -2.0668e-02,\n",
       "        -6.7172e-04,  2.0473e-02, -2.9267e-03,  2.3640e-02,  2.3624e-02,\n",
       "        -1.3084e-02,  1.1637e-02,  2.1812e-02, -6.1209e-03,  1.2740e-02,\n",
       "         6.9841e-03,  3.5421e-03, -1.8502e-02,  6.1026e-03, -8.8321e-03,\n",
       "         4.3214e-03, -1.5683e-02, -1.7253e-02,  1.3161e-02,  1.6371e-02,\n",
       "        -7.0652e-03, -2.4581e-02, -1.8443e-02,  8.5906e-03, -2.3088e-03,\n",
       "        -8.8641e-03,  3.3864e-03,  4.2166e-03,  1.4950e-02,  7.1524e-03,\n",
       "         1.3384e-02,  4.9487e-03,  2.2061e-02, -5.1756e-03,  1.9411e-02,\n",
       "        -2.3950e-02, -1.9553e-02, -2.2834e-02,  1.3409e-02, -2.3056e-02,\n",
       "         1.9252e-02, -1.6056e-02,  2.2745e-02,  7.2562e-04, -1.2180e-02,\n",
       "         1.3509e-02,  1.3997e-02,  1.3488e-02,  2.7645e-03,  1.4166e-03,\n",
       "         2.6231e-02, -2.7369e-02, -7.8723e-03, -1.1879e-02, -3.4895e-03,\n",
       "         2.0902e-02,  1.2352e-02, -4.8702e-02,  1.0115e-03,  2.0551e-02,\n",
       "         9.7993e-03,  4.7385e-03,  6.9045e-03,  1.2759e-02, -6.0339e-03,\n",
       "        -9.5646e-04, -3.5349e-03, -2.2549e-02,  7.4816e-03,  1.8219e-02,\n",
       "         1.7277e-02, -4.1283e-02,  9.4238e-04,  1.0710e-02,  3.0185e-02,\n",
       "        -2.8276e-02, -7.3518e-03, -6.1939e-04,  1.7559e-02, -2.3293e-02,\n",
       "         6.5164e-03, -1.2308e-02, -5.8904e-03,  1.5784e-03,  1.3586e-02,\n",
       "         8.5274e-03, -8.0722e-04, -4.7194e-04, -1.5454e-02, -1.2120e-02,\n",
       "        -6.1155e-03,  1.2969e-02,  1.2381e-02,  5.4678e-03, -3.1477e-02,\n",
       "        -5.3354e-03,  8.7158e-03,  7.9873e-03, -6.5966e-03,  4.3139e-03,\n",
       "        -5.7107e-03,  1.0297e-02, -1.7702e-05,  1.0606e-02, -1.7462e-02,\n",
       "         1.9799e-02,  5.1128e-03,  7.2834e-03, -1.1845e-02, -8.2144e-03,\n",
       "         2.8355e-02,  8.8271e-03, -9.9582e-05,  6.0243e-03, -2.0797e-02,\n",
       "         2.6334e-02, -7.4303e-03,  1.3735e-02, -1.0614e-02,  2.9173e-02,\n",
       "        -5.3931e-03,  6.4489e-03, -2.2334e-02, -1.7242e-02, -5.8175e-03,\n",
       "        -1.3505e-02, -1.9959e-02,  1.4064e-02,  1.2240e-02, -3.1030e-03,\n",
       "        -1.0900e-02, -1.0181e-02,  7.9565e-03,  2.0258e-03,  5.1391e-03,\n",
       "         2.2122e-02,  5.6625e-03,  2.6422e-02,  1.7275e-02,  1.9632e-02,\n",
       "        -7.4751e-03, -6.3665e-03,  5.4449e-03, -1.1073e-02, -9.9278e-05,\n",
       "        -2.0679e-03, -9.1023e-03,  9.4417e-03,  4.5740e-04,  2.7104e-02,\n",
       "        -2.4908e-02, -8.8115e-03, -1.4721e-02, -1.5577e-03, -1.2182e-02,\n",
       "         1.1662e-02,  2.4272e-02, -1.5465e-02,  5.1547e-03, -2.0314e-02,\n",
       "        -1.3826e-02, -1.7338e-02, -2.4986e-02, -4.9542e-03,  1.2320e-02,\n",
       "         3.2298e-02, -1.8333e-02,  8.4245e-04,  2.1725e-02,  3.6582e-03,\n",
       "         1.8704e-02, -6.0173e-03,  1.5939e-04,  7.0984e-03, -2.2068e-02,\n",
       "         3.4760e-02,  4.0923e-04, -1.9829e-02,  1.4743e-02, -1.3204e-02,\n",
       "         2.2237e-02,  3.2868e-02, -8.6183e-03, -2.6778e-02,  1.2065e-02,\n",
       "         7.1698e-03,  6.1762e-03,  1.2369e-02,  1.2723e-02, -6.7285e-03,\n",
       "         1.5520e-02,  4.0083e-02,  5.2534e-03,  8.1113e-03, -2.0882e-02,\n",
       "         1.1336e-02, -3.1061e-03,  6.9217e-03,  2.6734e-02, -3.4843e-02,\n",
       "        -1.4095e-02, -1.4068e-02,  7.9352e-03,  6.6631e-03, -4.0312e-03,\n",
       "        -2.6556e-02,  2.0214e-02,  1.1864e-02, -4.8301e-03,  2.9448e-03,\n",
       "         1.1213e-02, -2.9420e-03, -8.0387e-03,  2.1030e-02,  1.8896e-02,\n",
       "         3.0987e-02,  1.1895e-02, -2.9171e-03,  2.0557e-02,  2.7279e-03,\n",
       "        -2.3245e-02, -3.3197e-03, -1.8784e-03,  4.9393e-03, -2.0854e-03,\n",
       "        -1.3983e-03, -4.7537e-03, -3.2656e-02,  7.0653e-03, -1.3532e-02,\n",
       "        -9.9613e-03, -1.7447e-02, -1.6134e-02,  7.8123e-03, -2.5033e-02,\n",
       "        -1.9141e-02, -5.2730e-03, -1.7276e-02,  9.2858e-03, -3.3329e-03,\n",
       "         2.9654e-04, -2.0972e-02,  7.3528e-03, -2.0273e-03,  1.5063e-02,\n",
       "         1.0650e-02,  2.2873e-02, -7.2405e-03, -3.0977e-02,  5.4080e-03,\n",
       "         3.0254e-02, -5.6288e-03, -8.9885e-03,  2.8087e-03,  1.3201e-02,\n",
       "        -1.8427e-02, -1.0530e-02,  7.2621e-03,  1.4139e-02, -1.6867e-02,\n",
       "         6.2915e-03, -1.5106e-02, -5.6171e-02,  2.0584e-04, -3.7615e-02,\n",
       "         7.2365e-03,  3.9460e-02, -1.7386e-02,  5.8280e-03,  8.7508e-03,\n",
       "         1.4223e-02,  5.2078e-03, -3.0773e-02, -1.4211e-02, -3.3885e-02,\n",
       "        -6.1075e-03, -7.4425e-03, -1.9212e-03, -2.5376e-02,  6.8429e-03,\n",
       "        -1.6863e-02,  1.2359e-03, -7.3506e-03,  3.3583e-02,  2.6073e-02,\n",
       "         3.6853e-03,  2.9647e-02, -6.5526e-03,  1.5773e-02,  9.3750e-03,\n",
       "        -2.7697e-02, -6.2232e-03,  3.4556e-03,  1.5448e-02,  1.5153e-02,\n",
       "        -1.2855e-02, -8.7281e-03,  1.7279e-02,  8.6607e-03,  6.0227e-03,\n",
       "        -2.2224e-02,  2.3618e-03, -1.0651e-02, -2.7309e-02, -1.5886e-02,\n",
       "        -2.4016e-02, -2.1634e-02, -1.5941e-02,  7.6576e-03, -1.6788e-03,\n",
       "        -3.0255e-03,  1.3794e-03,  3.2629e-03, -1.2367e-02,  1.2910e-02,\n",
       "        -8.2178e-03, -2.7701e-03,  2.0563e-02,  9.4212e-04,  1.3475e-02,\n",
       "         1.0286e-02, -1.9219e-02,  1.8602e-02, -2.1198e-02, -3.7252e-03,\n",
       "        -2.0171e-02,  1.4370e-02, -1.1929e-02,  1.1082e-02, -2.3385e-02,\n",
       "        -1.2488e-02, -1.0542e-02, -2.6096e-02, -8.3948e-03,  1.6603e-02,\n",
       "         3.5164e-02, -1.0684e-02,  1.0932e-02, -2.8958e-03,  1.4103e-02,\n",
       "         1.1363e-02,  1.0658e-02,  1.4979e-03, -5.2952e-03, -2.5783e-03,\n",
       "        -1.7587e-02,  6.3837e-03, -1.2760e-02, -1.3824e-03,  8.9615e-03,\n",
       "         4.5580e-04, -2.0907e-02, -8.0227e-03, -1.6082e-02,  1.5819e-02,\n",
       "         1.7052e-02,  1.5020e-04,  9.4952e-03,  1.2992e-04,  2.7005e-02,\n",
       "        -1.2390e-03,  1.4518e-02, -9.8797e-03,  1.2378e-04, -9.0517e-03,\n",
       "        -6.3255e-03,  1.2462e-02, -6.9565e-03, -2.5079e-02,  1.2992e-02,\n",
       "        -3.3288e-03,  3.3488e-02,  1.2453e-02,  2.5309e-04,  8.1020e-03,\n",
       "        -2.3058e-03,  2.4013e-03,  2.5679e-02,  1.6609e-02,  1.0435e-02,\n",
       "        -1.0573e-02, -2.4966e-03, -7.6580e-03, -3.1558e-02, -2.3880e-03,\n",
       "        -1.6137e-02,  2.5647e-02, -1.5597e-02, -6.9348e-05, -6.4397e-03,\n",
       "        -3.6518e-03, -1.3942e-02,  6.1993e-03, -5.6389e-03,  1.0175e-02,\n",
       "        -9.5844e-03,  1.4555e-02, -1.4600e-02, -1.9525e-02, -1.2377e-02,\n",
       "        -1.8349e-02, -1.5220e-02,  5.7487e-03,  1.4635e-02, -6.3309e-03,\n",
       "         4.1885e-03,  6.5806e-03, -2.0814e-03,  1.3554e-02, -7.1452e-03,\n",
       "        -1.6063e-02,  1.9511e-02, -1.8748e-03, -2.3187e-02, -5.2322e-03,\n",
       "        -1.8407e-03, -1.2486e-02, -9.0428e-03, -3.0186e-02, -6.4230e-03,\n",
       "         8.6573e-03,  4.2880e-02,  1.4224e-02, -3.1076e-02,  6.9983e-04,\n",
       "        -1.4578e-03, -1.2993e-02,  3.7837e-03,  1.6045e-02, -2.2239e-02,\n",
       "        -6.4480e-03,  1.8523e-02,  1.3017e-02, -4.1386e-03,  2.0800e-02,\n",
       "         1.5711e-02,  5.6140e-03,  9.6229e-03,  8.1060e-03, -3.9216e-03,\n",
       "         1.1898e-02,  1.6187e-02, -1.5952e-02,  6.8478e-03, -1.7122e-02,\n",
       "         6.0422e-03,  1.6478e-02, -7.0226e-03, -3.0229e-02, -1.2967e-02,\n",
       "        -3.5226e-02,  1.6307e-02,  1.7449e-02,  3.1971e-03, -4.4580e-02,\n",
       "         1.3996e-02,  2.4550e-02, -2.0222e-03, -3.6965e-02,  4.4091e-03,\n",
       "        -6.2659e-03,  1.1496e-02, -1.1204e-02, -3.7791e-03,  1.7548e-02,\n",
       "        -1.7699e-02,  1.0883e-02, -3.4274e-03, -5.0328e-03,  3.0583e-03,\n",
       "         2.9778e-03,  1.7949e-03,  7.3280e-03, -6.9347e-03,  1.5783e-03,\n",
       "        -2.0175e-02,  5.6984e-03,  1.6116e-02, -4.8197e-03,  3.4322e-02,\n",
       "         1.3808e-04,  4.2976e-05, -1.0557e-02, -5.7288e-03,  9.1250e-03,\n",
       "         2.9986e-03, -5.3921e-03,  2.6803e-02,  2.4643e-02, -1.5793e-02,\n",
       "         1.2334e-02,  1.4607e-02,  8.9689e-03,  2.8939e-03, -3.8824e-03,\n",
       "        -5.5546e-03, -3.0235e-03, -2.9130e-02, -1.1008e-02, -1.1143e-02,\n",
       "        -2.4462e-02, -1.7904e-02, -4.0464e-03,  1.3746e-02, -1.2139e-03,\n",
       "         1.0502e-02, -1.0752e-02, -2.6262e-02,  5.5538e-03,  8.9968e-03,\n",
       "         8.2223e-04,  1.6705e-02, -8.9519e-03, -1.3860e-03, -1.3056e-02,\n",
       "         1.9211e-02,  1.8789e-03,  1.1189e-02, -6.1424e-04, -2.1604e-02,\n",
       "        -2.2795e-03, -1.3854e-02,  3.5380e-03, -2.0459e-02, -5.6942e-03,\n",
       "         1.1240e-02, -3.2689e-04, -1.5334e-02,  1.2912e-02,  7.0726e-03,\n",
       "        -2.6452e-03, -3.8886e-03,  3.8937e-03, -1.0482e-02,  1.7545e-02,\n",
       "        -5.5036e-03, -3.0839e-02,  4.0244e-03, -7.2722e-03,  2.6349e-04,\n",
       "        -1.8834e-02,  3.3224e-03,  5.1510e-03,  8.5166e-04,  1.3522e-02,\n",
       "        -2.3151e-03, -8.5860e-04, -1.2648e-02, -1.6390e-02, -8.5032e-03,\n",
       "         8.1336e-03,  1.0335e-03, -1.2154e-03, -1.0982e-02, -2.6311e-02,\n",
       "         4.8888e-03,  9.2687e-03,  2.8256e-02, -1.2525e-02, -1.1152e-02,\n",
       "        -6.0340e-04, -2.3681e-04,  2.8583e-02,  9.1067e-03,  1.9469e-02,\n",
       "         5.5957e-03,  5.9555e-03,  1.4561e-03, -9.5751e-03,  2.1983e-02,\n",
       "        -5.1971e-03, -1.6079e-02, -2.0609e-02, -1.2711e-02,  3.1143e-02,\n",
       "        -2.0769e-02, -2.0357e-02,  3.3225e-02,  1.2083e-02, -2.2756e-03,\n",
       "        -1.8310e-02, -1.3390e-02,  7.0640e-03])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
